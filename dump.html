<!-- 


<div align="center">
<img src="./assets/Images/reynolds3.png" width="580" height="350" />
<br />Figure 1: Swarm Algorithm Pipeline<br />
</div>

<ul>
  <li>
    <h2 id="input-image-sequence">Input Image sequence</h2>
    <p>Capture  stereo image pair at time T and T+1. The images are then processed to compensate for lens distortion. To simplify the task of disparity map computation stereo rectification is done so that epipolar lines become parallel to horizontal. In KITTI dataset the input images are already corrected for lens distortion and stereo rectified.</p>
  </li>
  <li>
    <h2 id="feature-detection">Feature Detection</h2>
    <p>Features are generated on left camera image at time T using FAST (Features from Accelerated Segment Test) corner detector. FAST is computationally less expensive than other feature detectors like SIFT and SURF. To accurately compute the motion between image frames, feature bucketing is used. The image is divided into several non-overlapping rectangles and a maximum number (10) of feature points  with highest response value are then selected from each bucket. There are two benefits of bucketing: i) Input features are well distributed throughout the image which results in higher accuracy in motion estimation. ii) Due to less number of features computation complexity of algorithm is reduced which is a requirstrongent in low-latency applications. Disparity map for time T is also generated using the left and right image pair.</p>
  </li>
</ul>

<div align="center">
<img src="./docs/1_FAST_features.png" width="900" height="300" />
Figure 2: FAST Features<br />
</div>

<ul>
  <li>
    <h2 id="feature-tracking">Feature Tracking</h2>
    <p>Features generated in previous step are then searched in image at time T+1. The original paper [1] does feature matching by computing the feature descriptors and then comparing thstrong from images at both time instances. More recent literature uses KLT (Kanade-Lucas-Tomasi) tracker for feature matching. Features from image at time T are tracked at time T+1 using a 15x15 search windows and 3 image pyramid level search. KLT tracker outputs the corresponding coordinates for each input feature and accuracy and error measure by which each feature was tracked. Feature points that are tracked with high error or lower accuracy are dropped from further computation.</p>
  </li>
</ul>

<div align="center">
<img src="./docs/2_keyPointsT1.png" width="900" height="300" />
Figure 3: Features at time T<br />
</div>

<div align="center">
<img src="./docs/3_keyPointsT2.png" width="900" height="300" />
Figure 4: KLT tracked features at time T+1<br />
</div>

<ul>
  <li>
    <h2 id="3d-point-cloud-generation">3D Point Cloud Generation</h2>
    <p>Now that we have the 2D points at time T and T+1, corresponding 3D points with respect to left camera are generated using disparity information and camera projection matrices. For each feature point a system of equations is formed for corresponding 3D coordinates (world coordinates) using left, right image pair and it is solved using singular value decomposition to obtain 3D points.</p>
  </li>
  <li>
    <h2 id="inlier-detection">Inlier Detection</h2>
    <p>Instead of an outlier rejection algorithm this paper uses an inlier detection algorithm which exploits the rigidity of scene points to find a subset of consistent 3D points at both time steps. The key idea here is the observation that although the absolute position of two feature points will be different at different time points the relative distance between them remains the same. If any such distance is not same, then either there is an error in 3D triangulation of at least one of the two features, or we have triangulated is moving, which we cannot use in the next step.</p>
  </li>
  <li>
    <h2 id="motion-estimation">Motion Estimation</h2>
    <p>Frame to frame camera motion is estimated by minimizing the image re-projection error for all matching feature points. Image re-projection here means that for a pair of corresponding matching points Ja and Jb at time T and T+1, there exits corresponding world coordinates Wa and Wb. The world coordinates are re-projected back into image using a transform (delta) to estimate the 2D points for complementary time step and the distance between the true and projected 2D point is minimized using Levenberg-Marquardt least square optimization.</p>
  </li>
</ul>

<div align="center">
<img src="./docs/reprojection.png" height="275" />
<br />Figure 5: Image Reprojection : Wb --&gt; Ja  and Wa --&gt; Jb <br />
</div> -->




<!-- </section>
<h1 id="results">Simulation Results</h1>
<p>We have implemented above algorithm using Python 3 and OpenCV 3.0 and source code is maintained <a href="https://github.com/cgarg92/Stereo-visual-odometry/">here</a>. KITTI visual odometry [2] dataset is used for evaluation. In the KITTI dataset the ground truth poses are given with respect to the zeroth frame of the camera. Following video shows a short demo of trajectory computed along with input video data.</p> -->
<!-- <video src="./docs/demoVideo.mp4" width="900" height="350" controls="" preload=""></video> -->
<!-- <div align="center">Demo Video </div> -->
<!-- <p><br /></p> -->
<!-- 
<h1 id="results">Simulation Results</h1> -->
<!-- <p>Description of the overall results and their significance.</p> -->

<!-- <div class="result-gifs">
  <img src="assets/gif/map_a1.gif" alt="Result GIF 1">
  <img src="assets/gif/map_a2.gif" alt="Result GIF 2">
</div> -->
<!-- <p>Figure 1: Description of Map A1 and A2 results.</p> -->



<!-- 
<p>Figure 6 illustrates computed trajectory for two sequences. For linear translational motion the algorithm tracks ground truth well, however for continuous turning motion such as going through a hair pin bend the correct angular motion is not computed which results in error throughout the latter estimates. For very fast translational motion the algorithm does not perform well because of lack of overlap between consecutive images.</p>

<div align="center">
  <div class="result-gifs">
    <img src="assets/gif/map_r1.gif" alt="Result GIF 1">
    <img src="assets/gif/map_r2.gif" alt="Result GIF 2">
</div>
<p>Figure 1: Description of Map A1 and A2 results.</p><br />
</div> -->


<!-- <div align="center">
    <div class="result-gifs">
      <img src="assets/gif/map_b1.gif" alt="Result GIF 1">
      <img src="assets/gif/map_b2.gif" alt="Result GIF 2">
    </div>
  Figure 6: Output trajectory for sequence 00 and 02 from KITTI dataset <br/>
  </div>
  
  
  <div align="center">
    <div class="result-gifs">
      <img src="assets/gif/map_c1.gif" alt="Result GIF 1">
      <img src="assets/gif/map_c2.gif" alt="Result GIF 2">
    </div>
  Figure 6: Output trajectory for sequence 00 and 02 from KITTI dataset <br/>
  </div>
  
  
  <div align="center">
    <div class="result-gifs">
      <img src="assets/gif/map_d1.gif" alt="Result GIF 1">
      <img src="assets/gif/see_through.png" alt="Result GIF 2">
    </div>
  Figure 6: Output trajectory for sequence 00 and 02 from KITTI dataset <br/>
  </div> -->


  
<!-- <p>This project focuses on implementing Craig Reynolds' behavioral rules for controlling a swarm of robots in a simulated environment. The implementation involves initially incorporating fundamental rules such as Separation, Alignment, and Cohesion, followed by the development of additional rules for Navigation to a specific point and Obstacle Avoidance. The robots, represented as omni-directional points in the Stage simulator, were tested in various scenarios to assess their behavior in different environments. The goal is to explore and understand the adaptability and performance of swarm behaviors using Reynolds' rules and additional algorithms in simulated settings. Simulation results are included.</p>

<p>Over the years, visual odometry has evolved from using stereo images to monocular imaging and now incorporating LiDAR laser information which has started to become mainstream in upcoming cars with self-driving capabilities. It is also a prerequisite for applications like obstacle detection, simultaneous localization and mapping (SLAM) and other tasks. Visual-SLAM (VSLAM) is a much more evolved variant of visual odometry which obtain global, consistent estimate of robot path. The path drift in VSLAM is reduced by identifying loop closures.</p> -->

<!-- <p>Some of the challenges encountered by visual odometry algorithms are:</p>
<ol>
  <li>Varying lighting conditions</li>
  <li>In-sufficient scene overlap between consecutive frames</li>
  <li>Lack of texture to accurately estimate motion</li>
</ol> -->
<!-- 
<h2 id="types-of-visual-odometry">Implemented Behaviours</h2>

<ul>
  <li>
    <h3 id="monocular-visual-odometry">Separation</h3>
    <p>A single camera is used to capture motion. Usually a five-point relative pose  estimation method is used to estimate motion, motion computed is on a relative scale. Typically used in hybrid methods where other sensor data is also available.</p>
  </li>
  <li>
    <h3 id="stereo-visual-odometry">Alignment</h3>
    <p>A calibrated stereo camera pair is used which helps compute the feature depth between images at various time points. Computed output is actual motion (on scale). If only faraway features are tracked then degenerates to monocular case.</p>
  </li>
  <li>
    <h3 id="stereo-visual-odometry">Cohesion</h3>
    <p>A calibrated stereo camera pair is used which helps compute the feature depth between images at various time points. Computed output is actual motion (on scale). If only faraway features are tracked then degenerates to monocular case.</p>
  </li>
  <li>
    <h3 id="stereo-visual-odometry">Seek</h3>
    <p>A calibrated stereo camera pair is used which helps compute the feature depth between images at various time points. Computed output is actual motion (on scale). If only faraway features are tracked then degenerates to monocular case.</p>
  </li>
  <li>
    <h3 id="stereo-visual-odometry">Arrival</h3>
    <p>A calibrated stereo camera pair is used which helps compute the feature depth between images at various time points. Computed output is actual motion (on scale). If only faraway features are tracked then degenerates to monocular case.</p>
  </li>
  <li>
    <h3 id="stereo-visual-odometry">Obstacle Avoidance</h3>
    <p>A calibrated stereo camera pair is used which helps compute the feature depth between images at various time points. Computed output is actual motion (on scale). If only faraway features are tracked then degenerates to monocular case.</p>
  </li>
</ul> -->

<!-- <h2 id="implemented-behaviours">Implemented Behaviours</h2> -->
<!-- 
<ul>
  <li>
    <h3 id="separation">Separation</h3>
    <p>This behavior ensures each robot maintains a certain distance from others to avoid collisions.</p>
    <div class="behavior-images">
      <img src="assets/gif/alignment.gif" alt="Separation Image 1">
      <img src="assets/gif/separation.gif" alt="Separation Image 1">
      <img src="assets/gif/Cohesion.gif" alt="Separation Image 1">
    </div>
  </li>

  <li>
    <h3 id="alignment">Alignment</h3>
    <p>Alignment behavior makes the robots align their direction of movement with their neighbors.</p>
  </li>
  <li>
    <h3 id="cohesion">Cohesion</h3>
    <p>Cohesion encourages robots to steer towards the average position of nearby robots, promoting group cohesion.</p>
  </li>
  <li>
    <h3 id="seek">Seek</h3>
    <p>This behavior guides robots to move towards a target point or area.</p>
  </li>
  <li>
    <h3 id="arrival">Arrival</h3>
    <p>Arrival behavior is designed for the robots to reach a designated position while slowing down as they arrive.</p>
  </li>
  <li>
    <h3 id="obstacle-avoidance">Obstacle Avoidance</h3>
    <p>Obstacle avoidance ensures that robots can identify and navigate around obstacles in their path.</p>
  </li>

  <li>
    <h3 id="separation">Steer to Avoid</h3>
    <p>This behavior ensures each robot maintains a certain distance from others to avoid collisions.</p>
    <div class="behavior-images">
      <img src="assets/gif/steer_to_avoid_1.gif" alt="Separation Image 1">
      <img src="assets/gif/steer_to_avoid_2.gif" alt="Separation Image 1">
      <img src="assets/gif/s.gif" alt="Separation Image 1">
    </div>
  </li>
  <li>
</ul> -->

